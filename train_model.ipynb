{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "import keras\n",
    "import argparse\n",
    "\n",
    "from keras.models    import Sequential\n",
    "from keras.models    import Model, load_model\n",
    "from keras.layers    import Dense, Dropout, Flatten, BatchNormalization, Input, Embedding, LSTM, GRU\n",
    "from keras.layers    import Conv2D, MaxPooling2D, AveragePooling2D, Bidirectional, Lambda\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras           import backend as K\n",
    "from keras.utils     import to_categorical\n",
    "\n",
    "from sklearn.utils           import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics         import log_loss\n",
    "from sklearn.preprocessing   import OneHotEncoder\n",
    "\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once a folder of TFRecords files have been created\n",
    "# create k-sets of train-test splits\n",
    "# store the train/test filenames per split in a .pkl file\n",
    "\n",
    "# load the train_sets \n",
    "train_sets = np.load('./train_sets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current index of train-test split\n",
    "\n",
    "train_test_index = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sets[train_test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to your TFRecords \n",
    "\n",
    "path = '../Final/tf_records/'\n",
    "\n",
    "img_shape    = (128,128)\n",
    "img_size     = 128\n",
    "num_channels = 1\n",
    "num_classes  = 5\n",
    "\n",
    "# retrieve all the TFRecord file paths\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "path_tfrecord =  sorted([\n",
    "                            path + '/' + f \n",
    "                            for f in listdir(path) \n",
    "                            if isfile(join(path, f))\n",
    "                        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should only occur once; Once a split has been made, save it\n",
    "\n",
    "# # construt train test split\n",
    "# # 5-folds ==> 80% training / 20% testing\n",
    "\n",
    "# # train and test sets will be stored so that each experiment may be run separately\n",
    "# # as opposed to performing model training (lengthy) for all 5 splits in one go \n",
    "\n",
    "# train_sets = []\n",
    "# test_sets  = []\n",
    "\n",
    "# kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "# for train_index, test_index in kf.split(path_tfrecord):\n",
    "    \n",
    "#     train_set = np.asarray(path_tfrecord)[train_index]\n",
    "#     test_set  = np.asarray(path_tfrecord)[test_index]\n",
    "    \n",
    "#     train_sets.append(train_set)\n",
    "#     test_sets.append(test_set)\n",
    "    \n",
    "# # dump splits to file (so that testing and training are consistent)\n",
    "\n",
    "# np.asarray(train_sets).dump('./train_sets.pkl')\n",
    "# np.asarray(test_sets).dump('./test_sets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/18_TFRecords_Dataset_API.ipynb\n",
    "\n",
    "# Data in the form of TFRecords needs to be parsed before it is consumed by model\n",
    "# In this case, the mp3 spectrograms and one hot encoded labels have been\n",
    "# serialized into binary strings\n",
    "\n",
    "# parse reads the serialized data\n",
    "\n",
    "def parse(serialized):\n",
    "    # Define a dict with the data-names and types we expect to find in the\n",
    "    # TFRecords file. It is a bit awkward that this needs to be specified again,\n",
    "    # because it could have been written in the header of the TFRecords file\n",
    "    # instead.\n",
    "    features =         {\n",
    "            'image': tf.FixedLenSequenceFeature([], tf.float32, allow_missing = True),\n",
    "            'label': tf.FixedLenSequenceFeature([], tf.float32, allow_missing = True)\n",
    "        }\n",
    "    # Parse the serialized data so we get a dict with our data.\n",
    "    parsed_example = tf.parse_single_example (\n",
    "                                                    serialized = serialized,\n",
    "                                                    features   = features\n",
    "                                                )\n",
    "    image = parsed_example['image']\n",
    "\n",
    "    # Get the label associated with the image.\n",
    "    label = parsed_example['label']\n",
    "\n",
    "    # The image and label are now correct TensorFlow types.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will create a tf.dataset for optimized consumption of training data\n",
    "# the returned image, label tuple represents a call to iterator.get_next()\n",
    "\n",
    "def create_dataset(filepath, epochs):\n",
    "    \n",
    "    # This works with arrays as well\n",
    "    dataset = tf.data.TFRecordDataset(filepath)\n",
    "    \n",
    "    # Maps the parser on every filepath in the array. You can set the number of parallel loaders here\n",
    "    dataset = dataset.map(parse, num_parallel_calls=8)\n",
    "    \n",
    "    # This dataset will go on forever\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # Set the number of datapoints you want to load and shuffle \n",
    "    dataset = dataset.shuffle(100)\n",
    "    \n",
    "    # Set the batchsize\n",
    "    dataset = dataset.batch(2)\n",
    "    \n",
    "    # Create an iterator\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    # Create your tf representation of the iterator\n",
    "    image, label = iterator.get_next()\n",
    "\n",
    "    # Bring your picture back in shape\n",
    "    image = tf.reshape(image, [-1, 128, 128, 1])\n",
    "    \n",
    "    # Create a one hot array for your labels\n",
    "    #label = tf.one_hot(label, 5)\n",
    "    label = tf.reshape(label, [-1,5])\n",
    "\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter init\n",
    "\n",
    "rows       = 128\n",
    "cols       = 128\n",
    "model_name = 'best_psy_cnn'\n",
    "epochs     = 20  # models have been shown to achieve approx 0.8 acc within 20 epochs\n",
    "keep_prob  = 0.2 # dropout probability -> tweakable hyperparam \n",
    "\n",
    "# create a train dataset with this current train split\n",
    "image, label = create_dataset(path_tfrecord, epochs)\n",
    "\n",
    "input_shape = (rows, cols)\n",
    "\n",
    "# init the input to Keras model as the tf.Dataset iterator\n",
    "inputs = Input(tensor = image)\n",
    "\n",
    "# model init\n",
    "x = BatchNormalization()(inputs)\n",
    "x = Conv2D(256, kernel_size=(4, cols), activation='relu', input_shape=input_shape)(x)\n",
    "shortcut = x\n",
    "\n",
    "# sizes of convolutional layers, filter window size, activation functions\n",
    "x = Conv2D(256, kernel_size=(4, 1), activation='relu', padding='same')(x)\n",
    "x = Conv2D(256, kernel_size=(4, 1), activation='relu', padding='same')(x)\n",
    "x = Conv2D(256, kernel_size=(4, 1), activation='relu', padding='same')(x)\n",
    "x = Conv2D(512, kernel_size=(4, 1), activation='relu', padding='same')(x)\n",
    "\n",
    "# usage of both average + max pooling, or either \n",
    "x1 = AveragePooling2D(pool_size=(125, 1))(keras.layers.concatenate([x, shortcut]))\n",
    "\n",
    "x2 = MaxPooling2D(pool_size=(125, 1))(keras.layers.concatenate([x, shortcut]))\n",
    "\n",
    "x = Dropout(keep_prob)(keras.layers.concatenate([x1, x2]))\n",
    "\n",
    "x = Dropout(keep_prob)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = Dropout(keep_prob)(x)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = Dropout(keep_prob)(x)\n",
    "\n",
    "pred = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "# usage of Adam, AdaDelta, RMSProp, Stochastic Gradient Descent\n",
    "model.compile(loss           = keras.losses.categorical_crossentropy,\n",
    "              optimizer      = keras.optimizers.Adam(),\n",
    "              metrics        = ['acc'],\n",
    "              target_tensors = [label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init checkpoints of model\n",
    "checkpoint     = ModelCheckpoint(model_name+'.h5', \n",
    "                                 monitor        = 'acc', \n",
    "                                 verbose        = 1, \n",
    "                                 save_best_only = True, \n",
    "                                 mode           = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init early stopping\n",
    "early_stop     = EarlyStopping(monitor='acc', \n",
    "                               patience=5, \n",
    "                               mode='max') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tensorboard callbacks (graph visualization)\n",
    "tensorboard    = keras.callbacks.TensorBoard(log_dir                = './logs', \n",
    "                                             histogram_freq         = 0, \n",
    "                                             batch_size             = 32, \n",
    "                                             write_graph            = True, \n",
    "                                             write_grads            = False, \n",
    "                                             write_images           = True, \n",
    "                                             embeddings_freq        = 0, \n",
    "                                             embeddings_layer_names = None, \n",
    "                                             embeddings_metadata    = None, \n",
    "                                             embeddings_data        = None, \n",
    "                                             update_freq            = 'epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to Marcin Mo≈ºejko - https://stackoverflow.com/a/43186440\n",
    "\n",
    "# init a Time Callback to record computation time per epoch metrics\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "        self.times_per_step = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.process_time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        \n",
    "        steps_per_epoch = 92\n",
    "        \n",
    "        result = time.process_time() - self.epoch_time_start\n",
    "        self.times.append(result)\n",
    "        self.times_per_step.append(result / steps_per_epoch)\n",
    "        \n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint, \n",
    "                  early_stop, \n",
    "                  tensorboard,\n",
    "                  time_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Concatenate'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expt_name = str(model.optimizer).split('.')[2].split(' ')[0]\n",
    "expt_name = str(model.layers[8]).split('.')[3].split(' ')[0]\n",
    "expt_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "92/92 [==============================] - 17s 183ms/step - loss: 1.4667 - acc: 0.3619\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.36192, saving model to best_psy_cnn.h5\n",
      "Epoch 2/20\n",
      "92/92 [==============================] - 15s 164ms/step - loss: 1.2624 - acc: 0.4518\n",
      "\n",
      "Epoch 00002: acc improved from 0.36192 to 0.45185, saving model to best_psy_cnn.h5\n",
      "Epoch 3/20\n",
      "92/92 [==============================] - 15s 162ms/step - loss: 1.0807 - acc: 0.5345\n",
      "\n",
      "Epoch 00003: acc improved from 0.45185 to 0.53453, saving model to best_psy_cnn.h5\n",
      "Epoch 4/20\n",
      "92/92 [==============================] - 15s 161ms/step - loss: 1.0062 - acc: 0.5859\n",
      "\n",
      "Epoch 00004: acc improved from 0.53453 to 0.58591, saving model to best_psy_cnn.h5\n",
      "Epoch 5/20\n",
      "92/92 [==============================] - 15s 162ms/step - loss: 0.9218 - acc: 0.6297\n",
      "\n",
      "Epoch 00005: acc improved from 0.58591 to 0.62971, saving model to best_psy_cnn.h5\n",
      "Epoch 6/20\n",
      "92/92 [==============================] - 15s 164ms/step - loss: 0.9037 - acc: 0.6406\n",
      "\n",
      "Epoch 00006: acc improved from 0.62971 to 0.64058, saving model to best_psy_cnn.h5\n",
      "Epoch 7/20\n",
      "92/92 [==============================] - 15s 164ms/step - loss: 0.8611 - acc: 0.6701\n",
      "\n",
      "Epoch 00007: acc improved from 0.64058 to 0.67011, saving model to best_psy_cnn.h5\n",
      "Epoch 8/20\n",
      "92/92 [==============================] - 15s 161ms/step - loss: 0.9048 - acc: 0.6520\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.67011\n",
      "Epoch 9/20\n",
      "92/92 [==============================] - 15s 160ms/step - loss: 0.8378 - acc: 0.6760\n",
      "\n",
      "Epoch 00009: acc improved from 0.67011 to 0.67598, saving model to best_psy_cnn.h5\n",
      "Epoch 10/20\n",
      "92/92 [==============================] - 15s 160ms/step - loss: 0.7940 - acc: 0.6989\n",
      "\n",
      "Epoch 00010: acc improved from 0.67598 to 0.69895, saving model to best_psy_cnn.h5\n",
      "Epoch 11/20\n",
      "92/92 [==============================] - 15s 160ms/step - loss: 0.8037 - acc: 0.6912\n",
      "\n",
      "Epoch 00011: acc did not improve from 0.69895\n",
      "Epoch 12/20\n",
      "92/92 [==============================] - 15s 161ms/step - loss: 0.7595 - acc: 0.7073\n",
      "\n",
      "Epoch 00012: acc improved from 0.69895 to 0.70732, saving model to best_psy_cnn.h5\n",
      "Epoch 13/20\n",
      "92/92 [==============================] - 15s 161ms/step - loss: 0.7274 - acc: 0.7183\n",
      "\n",
      "Epoch 00013: acc improved from 0.70732 to 0.71826, saving model to best_psy_cnn.h5\n",
      "Epoch 14/20\n",
      "92/92 [==============================] - 15s 163ms/step - loss: 0.7314 - acc: 0.7220\n",
      "\n",
      "Epoch 00014: acc improved from 0.71826 to 0.72199, saving model to best_psy_cnn.h5\n",
      "Epoch 15/20\n",
      "92/92 [==============================] - 15s 161ms/step - loss: 0.6853 - acc: 0.7386\n",
      "\n",
      "Epoch 00015: acc improved from 0.72199 to 0.73862, saving model to best_psy_cnn.h5\n",
      "Epoch 16/20\n",
      "92/92 [==============================] - 15s 161ms/step - loss: 0.6475 - acc: 0.7555\n",
      "\n",
      "Epoch 00016: acc improved from 0.73862 to 0.75554, saving model to best_psy_cnn.h5\n",
      "Epoch 17/20\n",
      "92/92 [==============================] - 15s 161ms/step - loss: 0.6709 - acc: 0.7463\n",
      "\n",
      "Epoch 00017: acc did not improve from 0.75554\n",
      "Epoch 18/20\n",
      "92/92 [==============================] - 15s 161ms/step - loss: 0.6125 - acc: 0.7708\n",
      "\n",
      "Epoch 00018: acc improved from 0.75554 to 0.77083, saving model to best_psy_cnn.h5\n",
      "Epoch 19/20\n",
      "92/92 [==============================] - 15s 161ms/step - loss: 0.6464 - acc: 0.7508\n",
      "\n",
      "Epoch 00019: acc did not improve from 0.77083\n",
      "Epoch 20/20\n",
      "92/92 [==============================] - 15s 161ms/step - loss: 0.5679 - acc: 0.7817\n",
      "\n",
      "Epoch 00020: acc improved from 0.77083 to 0.78174, saving model to best_psy_cnn.h5\n"
     ]
    }
   ],
   "source": [
    "# train model with current hyperparameters \n",
    "\n",
    "# steps per epoch = 92 since there 184 TFRecords (each with 150 shuffled spectrograms-\n",
    "# 30 of each genre), and the iterator retrieves batches of 2 TFRecords\n",
    "# So for each epoch, all 184 TFRecords are consumed \n",
    "\n",
    "history = model.fit(\\\n",
    "                            batch_size       = None,\n",
    "                            epochs           = epochs,\n",
    "                            verbose          = 1,\n",
    "                            shuffle          = True,\n",
    "                            callbacks        = callbacks_list,\n",
    "                            steps_per_epoch  = 92\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = time_callback.times\n",
    "times_per_step = time_callback.times_per_step\n",
    "acc = history.history['acc']\n",
    "epoch_col = [i for i in range(1,21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init result dataframe\n",
    "\n",
    "result_df = pd.DataFrame(columns = ['epoch', 'acc', 'time'])\n",
    "result_df['epoch'] = epoch_col\n",
    "result_df['acc']   = acc\n",
    "result_df['time']  = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/pool/pool_MaxPooling2Dfold_5.csv\n"
     ]
    }
   ],
   "source": [
    "# save experiment training results for current fold to csv\n",
    "\n",
    "expt_folder_name = 'pool/'\n",
    "results_name     = ('pool_' + \n",
    "                    str(expt_name).replace('.','') + \n",
    "                    'fold_' + \n",
    "                    str(train_test_index+1) + \n",
    "                    '.csv')\n",
    "results_path     = './results/' + expt_folder_name + results_name\n",
    "\n",
    "print(results_path)\n",
    "\n",
    "result_df.to_csv(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 1)  4           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 125, 1, 256)  131328      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 125, 1, 256)  262400      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 125, 1, 256)  262400      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 125, 1, 256)  262400      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 125, 1, 512)  524800      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 125, 1, 768)  0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 768)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 1, 768)    0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 768)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         1574912     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2048)         4196352     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2048)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5)            10245       dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,224,841\n",
      "Trainable params: 7,224,839\n",
      "Non-trainable params: 2\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
